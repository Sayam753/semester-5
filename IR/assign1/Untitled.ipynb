{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-05T17:28:17.541145Z",
     "iopub.status.busy": "2020-09-05T17:28:17.540748Z",
     "iopub.status.idle": "2020-09-05T17:28:17.546471Z",
     "shell.execute_reply": "2020-09-05T17:28:17.544985Z",
     "shell.execute_reply.started": "2020-09-05T17:28:17.541106Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-05T18:09:41.857799Z",
     "iopub.status.busy": "2020-09-05T18:09:41.857454Z",
     "iopub.status.idle": "2020-09-05T18:09:41.869615Z",
     "shell.execute_reply": "2020-09-05T18:09:41.868152Z",
     "shell.execute_reply.started": "2020-09-05T18:09:41.857760Z"
    }
   },
   "outputs": [],
   "source": [
    "posting_list = dict()\n",
    "vocab = dict()\n",
    "with open(\"posting_list.json\", \"w\") as file:\n",
    "    json.dump(posting_list, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-05T17:18:49.143009Z",
     "iopub.status.busy": "2020-09-05T17:18:49.142196Z",
     "iopub.status.idle": "2020-09-05T17:18:49.178131Z",
     "shell.execute_reply": "2020-09-05T17:18:49.166071Z",
     "shell.execute_reply.started": "2020-09-05T17:18:49.142957Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = (\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"ain\", \"all\", \"am\", \"an\",\n",
    "              \"and\", \"any\", \"are\", \"aren\", \"aren't\", \"as\", \"at\", \"be\", \"because\", \"been\",\n",
    "              \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can\", \"couldn\",\n",
    "              \"couldn't\", \"d\", \"did\", \"didn\", \"didn't\", \"do\", \"does\", \"doesn\", \"doesn't\",\n",
    "              \"doing\", \"don\", \"don't\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\",\n",
    "              \"further\", \"had\", \"hadn\", \"hadn't\", \"has\", \"hasn\", \"hasn't\", \"have\", \"haven\",\n",
    "              \"haven't\", \"having\", \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\", \"himself\",\n",
    "              \"his\", \"how\", \"i\", \"if\", \"in\", \"into\", \"is\", \"isn\", \"isn't\", \"it\", \"it's\", \"its\",\n",
    "              \"itself\", \"just\", \"ll\", \"m\", \"ma\", \"me\", \"mightn\", \"mightn't\", \"more\", \"most\", \n",
    "              \"mustn\", \"mustn't\", \"my\", \"myself\", \"needn\", \"needn't\", \"no\", \"nor\", \"not\", \"now\",\n",
    "              \"o\", \"of\", \"off\", \"on\", \"once\", \"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\",\n",
    "              \"out\", \"over\", \"own\", \"re\", \"s\", \"same\", \"shan\", \"shan't\", \"she\", \"she's\", \"should\",\n",
    "              \"should've\", \"shouldn\", \"shouldn't\", \"so\", \"some\", \"such\", \"t\", \"than\", \"that\", \n",
    "              \"that'll\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"these\",\n",
    "              \"they\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"ve\", \"very\",\n",
    "              \"was\", \"wasn\", \"wasn't\", \"we\", \"were\", \"weren\", \"weren't\", \"what\", \"when\", \"where\",\n",
    "              \"which\", \"while\", \"who\", \"whom\", \"why\", \"will\", \"with\", \"won\", \"won't\", \"wouldn\",\n",
    "              \"wouldn't\", \"y\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\",\n",
    "              \"yourself\", \"yourselves\", \"could\", \"he'd\", \"he'll\", \"he's\", \"here's\", \"how's\", \"i'd\",\n",
    "              \"i'll\", \"i'm\", \"i've\", \"let's\", \"ought\", \"she'd\", \"she'll\", \"that's\", \"there's\", \"they'd\",\n",
    "              \"they'll\", \"they're\", \"they've\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"what's\", \"when's\",\n",
    "              \"where's\", \"who's\", \"why's\", \"would\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-05T18:10:09.936639Z",
     "iopub.status.busy": "2020-09-05T18:10:09.936239Z",
     "iopub.status.idle": "2020-09-05T18:10:26.761411Z",
     "shell.execute_reply": "2020-09-05T18:10:26.760232Z",
     "shell.execute_reply.started": "2020-09-05T18:10:09.936594Z"
    }
   },
   "outputs": [],
   "source": [
    "for file_index in range(101, 282):\n",
    "    with open(f\"th-dataset/{file_index}.txt\") as file:\n",
    "        # Preprocessing\n",
    "        data = file.readlines()[0].strip()\n",
    "        data = re.sub(\"\\d+\", \"\", data)  # Remove digits\n",
    "        data = re.sub(\"[^\\w\\s]\", \" \", data)  # Remove punctuation\n",
    "        data = re.findall('[A-Za-z]+', data)  # Tokenize words\n",
    "        data_dict = Counter(data)  # Create data dict\n",
    "        \n",
    "        # Removing stop words\n",
    "        stop_words_in_data = set(data_dict).intersection(stop_words)\n",
    "        for words in stop_words_in_data:\n",
    "            del data_dict[words]\n",
    "    \n",
    "    # Update the posting list\n",
    "    with open(\"posting_list.json\", \"r\") as file:\n",
    "        posting_list = json.load(file)\n",
    "        \n",
    "        # Merging the dictionaries\n",
    "        for word, count in data_dict.items():\n",
    "            if word in posting_list:\n",
    "                posting_list[word].append((file_index, count))\n",
    "            else:\n",
    "                posting_list[word] = [(file_index, count)]\n",
    "\n",
    "    # Rewrite the posting list\n",
    "    with open(\"posting_list.json\", \"w\") as file:\n",
    "        json.dump(posting_list, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-05T18:10:29.670591Z",
     "iopub.status.busy": "2020-09-05T18:10:29.670249Z",
     "iopub.status.idle": "2020-09-05T18:10:29.676440Z",
     "shell.execute_reply": "2020-09-05T18:10:29.675101Z",
     "shell.execute_reply.started": "2020-09-05T18:10:29.670550Z"
    }
   },
   "outputs": [],
   "source": [
    "os.remove(\"posting_list.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-05T18:10:31.194315Z",
     "iopub.status.busy": "2020-09-05T18:10:31.193929Z",
     "iopub.status.idle": "2020-09-05T18:10:31.284346Z",
     "shell.execute_reply": "2020-09-05T18:10:31.281963Z",
     "shell.execute_reply.started": "2020-09-05T18:10:31.194274Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"output.txt\", \"w\") as file:\n",
    "    for size, (key, value) in zip(range(len(posting_list)), posting_list.items()):\n",
    "        vocab[key] = size\n",
    "        data = \", \".join(map(lambda doc: f\"{doc[0]}={doc[1]}\", value))\n",
    "        data = f\"{key} ({len(value)}) ==> \" + \"{\" + data + \"}\\n\"\n",
    "        file.writelines(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-05T18:10:32.394764Z",
     "iopub.status.busy": "2020-09-05T18:10:32.394362Z",
     "iopub.status.idle": "2020-09-05T18:10:32.406973Z",
     "shell.execute_reply": "2020-09-05T18:10:32.404946Z",
     "shell.execute_reply.started": "2020-09-05T18:10:32.394718Z"
    }
   },
   "outputs": [],
   "source": [
    "def query(word):\n",
    "    if word in posting_list:\n",
    "        with open(\"output.txt\", \"r\") as file:\n",
    "            print(file.readlines()[vocab[word]])\n",
    "    else:\n",
    "        print(\"Word not found in corpora.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-05T18:10:39.474236Z",
     "iopub.status.busy": "2020-09-05T18:10:39.473874Z",
     "iopub.status.idle": "2020-09-05T18:10:39.498148Z",
     "shell.execute_reply": "2020-09-05T18:10:39.495956Z",
     "shell.execute_reply.started": "2020-09-05T18:10:39.474198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barcelona (11) ==> {101=1, 104=1, 132=1, 144=1, 161=3, 162=1, 163=1, 212=1, 230=3, 235=1, 244=1}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query('Barcelona')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
